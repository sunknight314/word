"""
AIæ®µè½åˆ†ææœåŠ¡ - é‡æ„åçš„ç»Ÿä¸€æ¶æ„
"""

import os
import json
from datetime import datetime
from typing import Dict, List, Any, Tuple
from .ai_client_base import AIClientBase
from .document_processor import DocumentProcessor


class AIAnalyzer(AIClientBase):
    """AIæ®µè½åˆ†ææœåŠ¡ç±»"""
    
    def __init__(self, api_base_url: str = None, api_key: str = None, model: str = None, model_config: str = None):
        """
        åˆå§‹åŒ–AIåˆ†æå™¨
        
        Args:
            api_base_url: APIåŸºç¡€URL
            api_key: APIå¯†é’¥
            model: æ¨¡å‹åç§°
            model_config: æ¨¡å‹é…ç½®åç§°
        """
        # åˆå§‹åŒ–åŸºç±»
        super().__init__(api_base_url, api_key, model, model_config)
        
        # åˆå§‹åŒ–æ–‡æ¡£å¤„ç†å™¨
        self.document_processor = DocumentProcessor()
        
        # ç»“æœä¿å­˜é…ç½®
        self.save_results = True
        self.results_dir = "ai_analysis_results"
        self._ensure_results_dir()
    
    def get_prompts(self, input_data: List[Dict]) -> Tuple[str, str]:
        """
        è·å–æ®µè½åˆ†æçš„ç³»ç»Ÿå’Œç”¨æˆ·æç¤ºè¯
        
        Args:
            input_data: æ®µè½æ•°æ®åˆ—è¡¨
            
        Returns:
            (system_prompt, user_prompt) å…ƒç»„
        """
        from .ai_prompts import get_paragraph_analysis_prompt
        return get_paragraph_analysis_prompt(input_data)
    
    async def process_ai_response(self, ai_response: str) -> Dict[str, Any]:
        """
        å¤„ç†AIå“åº”ï¼Œè§£ææ®µè½åˆ†æç»“æœ
        
        Args:
            ai_response: AIåŸå§‹å“åº”
            
        Returns:
            å¤„ç†åçš„åˆ†æç»“æœ
        """
        try:
            print("ğŸ” å¼€å§‹æå–æ®µè½åˆ†æJSON...")
            json_content = self.extract_json_from_content(ai_response)
            print(f"ğŸ“‹ æå–çš„JSONé•¿åº¦: {len(json_content)} å­—ç¬¦")
            
            analysis_result = json.loads(json_content)
            print("âœ… æ®µè½åˆ†æJSONè§£ææˆåŠŸ")
            
            return {
                "success": True,
                "analysis_result": analysis_result.get("analysis_result", []),
                "raw_response": ai_response
            }
            
        except json.JSONDecodeError as e:
            print(f"âŒ æ®µè½åˆ†æJSONè§£æå¤±è´¥: {str(e)}")
            if 'json_content' in locals():
                print(f"ğŸ” å°è¯•è§£æçš„å†…å®¹: {json_content}")
            else:
                print("ğŸ” json_contentå˜é‡æœªå®šä¹‰")
            return {
                "success": False,
                "error": f"JSONè§£æå¤±è´¥: {str(e)}",
                "raw_response": ai_response
            }
    
    async def analyze_paragraphs(self, paragraphs_data: List[Dict]) -> Dict[str, Any]:
        """
        åˆ†ææ®µè½ç±»å‹ï¼ˆæ”¯æŒåˆ†æ‰¹å¤„ç†ï¼‰
        
        Args:
            paragraphs_data: æ®µè½æ•°æ®åˆ—è¡¨ï¼ŒåŒ…å«paragraph_numberå’Œpreview_text
            
        Returns:
            åˆ†æç»“æœå­—å…¸
        """
        try:
            print(f"ğŸ“„ å¼€å§‹åˆ†æ {len(paragraphs_data)} ä¸ªæ®µè½...")
            
            # åˆ¤æ–­æ˜¯å¦éœ€è¦åˆ†æ‰¹å¤„ç†
            if len(paragraphs_data) > 200:
                print(f"ğŸ“¦ æ®µè½æ•°é‡è¾ƒå¤šï¼Œå°†è¿›è¡Œåˆ†æ‰¹å¤„ç†...")
                return await self._analyze_paragraphs_in_batches(paragraphs_data)
            
            # ç›´æ¥ä½¿ç”¨AIåˆ†æï¼Œä¸€æ¬¡æ€§å¤„ç†
            result = await self.analyze(paragraphs_data)
            
            # ä¿å­˜åˆ†æç»“æœ
            if self.save_results and result.get("success"):
                self._save_analysis_result(result, paragraphs_data)
            
            return result
            
        except Exception as e:
            import traceback
            error_detail = f"{str(e)}\n{traceback.format_exc()}"
            error_result = {
                "success": False,
                "error": error_detail,
                "total_paragraphs": len(paragraphs_data)
            }
            
            # å³ä½¿å¤±è´¥ä¹Ÿä¿å­˜é”™è¯¯ä¿¡æ¯
            if self.save_results:
                self._save_analysis_result(error_result, paragraphs_data, is_error=True)
            
            return error_result
    
    async def _analyze_paragraphs_in_batches(self, paragraphs_data: List[Dict]) -> Dict[str, Any]:
        """
        åˆ†æ‰¹å¤„ç†æ®µè½ï¼Œå¸¦é‡å ä¸Šä¸‹æ–‡
        
        Args:
            paragraphs_data: æ®µè½æ•°æ®åˆ—è¡¨
            
        Returns:
            åˆå¹¶åçš„åˆ†æç»“æœ
        """
        batch_size = 100  # æ¯æ‰¹å¤§å°
        overlap_size = 10  # é‡å æ®µè½æ•°
        
        batches = []
        i = 0
        
        # åˆ›å»ºæ‰¹æ¬¡
        while i < len(paragraphs_data):
            # è®¡ç®—æ‰¹æ¬¡çš„ç»“æŸä½ç½®
            end = min(i + batch_size, len(paragraphs_data))
            
            # å¦‚æœä¸æ˜¯æœ€åä¸€æ‰¹ï¼ŒåŒ…å«é‡å 
            if end < len(paragraphs_data):
                batch = paragraphs_data[i:end + overlap_size]
            else:
                batch = paragraphs_data[i:end]
            
            batches.append({
                'data': batch,
                'start_index': i,
                'end_index': end,
                'has_overlap': end < len(paragraphs_data)
            })
            
            i = end
        
        print(f"ğŸ“¦ åˆ†æˆ {len(batches)} æ‰¹è¿›è¡Œå¤„ç†")
        
        # å¤„ç†æ¯ä¸ªæ‰¹æ¬¡
        all_results = []
        batch_errors = []
        
        for idx, batch_info in enumerate(batches):
            print(f"ğŸ”„ å¤„ç†ç¬¬ {idx + 1}/{len(batches)} æ‰¹...")
            try:
                batch_result = await self.analyze(batch_info['data'])
                
                if batch_result.get("success"):
                    analysis_results = batch_result.get("analysis_result", [])
                    
                    # å¦‚æœæœ‰é‡å ï¼Œå»é™¤é‡å éƒ¨åˆ†çš„åˆ†æç»“æœ
                    if batch_info['has_overlap'] and idx < len(batches) - 1:
                        # åªä¿ç•™åˆ°åŸå§‹æ‰¹æ¬¡å¤§å°çš„ç»“æœ
                        analysis_results = analysis_results[:batch_size]
                    
                    all_results.extend(analysis_results)
                else:
                    batch_errors.append(f"æ‰¹æ¬¡ {idx + 1} å¤±è´¥: {batch_result.get('error', 'æœªçŸ¥é”™è¯¯')}")
                    
            except Exception as e:
                batch_errors.append(f"æ‰¹æ¬¡ {idx + 1} å¼‚å¸¸: {str(e)}")
        
        # æ„å»ºæœ€ç»ˆç»“æœ
        if all_results:
            final_result = {
                "success": True,
                "analysis_result": all_results,
                "total_paragraphs": len(paragraphs_data),
                "batch_info": {
                    "total_batches": len(batches),
                    "batch_size": batch_size,
                    "overlap_size": overlap_size
                }
            }
            
            if batch_errors:
                final_result["partial_errors"] = batch_errors
            
            # ä¿å­˜åˆ†æç»“æœ
            if self.save_results:
                self._save_analysis_result(final_result, paragraphs_data)
            
            return final_result
        else:
            return {
                "success": False,
                "error": "æ‰€æœ‰æ‰¹æ¬¡å¤„ç†å¤±è´¥",
                "batch_errors": batch_errors,
                "total_paragraphs": len(paragraphs_data)
            }
    
    def _ensure_results_dir(self):
        """ç¡®ä¿ç»“æœä¿å­˜ç›®å½•å­˜åœ¨"""
        if not os.path.exists(self.results_dir):
            os.makedirs(self.results_dir)
            print(f"ğŸ“ åˆ›å»ºAIåˆ†æç»“æœç›®å½•: {self.results_dir}")
    
    def _save_analysis_result(self, result: Dict[str, Any], paragraphs_data: List[Dict], is_error: bool = False):
        """
        ä¿å­˜AIåˆ†æç»“æœåˆ°JSONæ–‡ä»¶
        
        Args:
            result: åˆ†æç»“æœ
            paragraphs_data: åŸå§‹æ®µè½æ•°æ®
            is_error: æ˜¯å¦ä¸ºé”™è¯¯ç»“æœ
        """
        try:
            # ç”Ÿæˆæ–‡ä»¶å
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            status = "error" if is_error else "success"
            filename = f"ai_analysis_{status}_{timestamp}.json"
            filepath = os.path.join(self.results_dir, filename)
            
            # å‡†å¤‡ä¿å­˜çš„æ•°æ®
            save_data = {
                "timestamp": datetime.now().isoformat(),
                "model": self.model,
                "status": status,
                "input_data": {
                    "total_paragraphs": len(paragraphs_data),
                    "paragraphs": paragraphs_data
                },
                "analysis_result": result
            }
            
            # ä¿å­˜åˆ°æ–‡ä»¶
            with open(filepath, 'w', encoding='utf-8') as f:
                json.dump(save_data, f, ensure_ascii=False, indent=2)
            
            print(f"ğŸ’¾ AIåˆ†æç»“æœå·²ä¿å­˜: {filepath}")
            
        except Exception as e:
            print(f"âš ï¸ ä¿å­˜AIåˆ†æç»“æœå¤±è´¥: {str(e)}")